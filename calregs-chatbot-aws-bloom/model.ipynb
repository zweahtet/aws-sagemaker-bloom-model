{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9448601e",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe943b8",
   "metadata": {},
   "source": [
    "- [SageMaker Text Generation Demo](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart_text_generation/Amazon_JumpStart_Text_Generation.ipynb)\n",
    "- This is the demo that i am using [SageMaker Sentence Pair Classification Demo](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart_sentence_pair_classification/Amazon_JumpStart_Sentence_Pair_Classification.ipynb)\n",
    "- [Fine-tune and host Hugging Face BERT models on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker/)\n",
    "- [SageMaker Built-in NLP models](https://sagemaker.readthedocs.io/en/stable/algorithms/text/sentence_pair_classification_hugging_face.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf3eb5",
   "metadata": {},
   "source": [
    "Before running the code, you will need an AWS account and set up SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fabee",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8e5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb5785",
   "metadata": {},
   "source": [
    "To train and host on Amazon SageMaker, we need to setup and authenticate the use of AWS services. Here, we use **the execution role associated with the current notebook instance** as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. (To check if you have the execution role, Go to AWS SageMaker -> Notebook (left-side nav) -> Choose your notebook instance -> Under Permissions and encryption -> See IAM role ARN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854e06f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arn:aws:iam::544669270813:role/service-role/AmazonSageMaker-ExecutionRole-20230307T225001',\n",
       " 'us-east-1',\n",
       " <sagemaker.session.Session at 0x7fef90daf280>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permissions and environment variables\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "aws_role, aws_region, sess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b3034",
   "metadata": {},
   "source": [
    "### Select a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3c4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"huggingface-spc-bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc797a",
   "metadata": {},
   "source": [
    "[Optional] Select a different JumpStart model. Here, we download jumpstart model_manifest file from the jumpstart s3 bucket, filter-out all the Sentence Pair Classification models and select a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479d70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "# download JumpStart model_manifest file.\n",
    "boto3.client(\"s3\").download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\", \"models_manifest.json\", \"models_manifest.json\"\n",
    ")\n",
    "with open(\"models_manifest.json\", \"rb\") as json_file:\n",
    "    model_list = json.load(json_file)\n",
    "\n",
    "# filter-out all the Sentence Pair Classification models from the manifest list.\n",
    "spc_models_all_versions, spc_models = [\n",
    "    model[\"model_id\"] for model in model_list if \"-spc-\" in model[\"model_id\"]\n",
    "], []\n",
    "[spc_models.append(model) for model in spc_models_all_versions if model not in spc_models]\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=spc_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729ea91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ded9f4b9284a5e965a0d48ecdfce72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select a model', index=3, layout=Layout(width='max-content'), options=('huggingface-spc-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose model for inference\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1295d789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('huggingface-spc-bert-base-uncased', '*')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"*\"\n",
    "model_id, model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf25293",
   "metadata": {},
   "source": [
    "### Run inference on the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626e5ac",
   "metadata": {},
   "source": [
    "#### Retreive JumpStart Artifacts & deploy an endpoint\n",
    "\n",
    "This will take a few minutes to fetch the model from sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2cbc1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# model_version=\"*\" fetches the latest version of the model.\n",
    "infer_model_id, infer_model_version = model_dropdown.value, \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{infer_model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=infer_model_id,\n",
    "    model_version=infer_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, script_scope=\"inference\"\n",
    ")\n",
    "# Retrieve the base model uri.\n",
    "base_model_uri = model_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, model_scope=\"inference\"\n",
    ")\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the SageMaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=base_model_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "# deploy the Model.\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd904f",
   "metadata": {},
   "source": [
    "#### Example input sentences for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b6a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pair1 = [\n",
    "    \"How many octaves does Beyonce have?\",\n",
    "    \"Beyoncé's vocal range spans four octaves.\",\n",
    "]\n",
    "sentence_pair2 = [\n",
    "    \"How many octaves does Beyonce have?\",\n",
    "    \"While another critic says she is a \"\n",
    "    \"Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6629658",
   "metadata": {},
   "source": [
    "#### Query endpoint and parse response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054ae23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "Input text: '['How many octaves does Beyonce have?', \"Beyoncé's vocal range spans four octaves.\"]'\n",
      "Model prediction: [2.8040263652801514, -3.48968768119812]\n",
      "Labels: ['entail', 'no_entail']\n",
      "Predicted Label: \u001b[1mentail\u001b[0m\n",
      "\n",
      "Inference:\n",
      "Input text: '['How many octaves does Beyonce have?', 'While another critic says she is a Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key.']'\n",
      "Model prediction: [-2.672070264816284, 3.6076502799987793]\n",
      "Labels: ['entail', 'no_entail']\n",
      "Predicted Label: \u001b[1mno_entail\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "def query_endpoint(encoded_text):\n",
    "    response = base_model_predictor.predict(\n",
    "        encoded_text, {\"ContentType\": \"application/list-text\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response)\n",
    "    probabilities, labels, predicted_label = (\n",
    "        model_predictions[\"probabilities\"],\n",
    "        model_predictions[\"labels\"],\n",
    "        model_predictions[\"predicted_label\"],\n",
    "    )\n",
    "    return probabilities, labels, predicted_label\n",
    "\n",
    "\n",
    "for sentence_pair in [sentence_pair1, sentence_pair2]:\n",
    "    query_response = query_endpoint(json.dumps(sentence_pair).encode(\"utf-8\"))\n",
    "    probabilities, labels, predicted_label = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"Input text: '{sentence_pair}'{newline}\"\n",
    "        f\"Model prediction: {probabilities}{newline}\"\n",
    "        f\"Labels: {labels}{newline}\"\n",
    "        f\"Predicted Label: {bold}{predicted_label}{unbold}{newline}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc1f90",
   "metadata": {},
   "source": [
    "#### Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586dd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b870984",
   "metadata": {},
   "source": [
    "### Finetune the pre-trained model on a custom dataset\n",
    "\n",
    "Previously, we saw how to run inference on a pre-trained model, which was fine-tuned on QNLI dataset. Next, we discuss how a model can be finetuned to a custom dataset.\n",
    "\n",
    "The Text Embedding model can be fine-tuned on any sentence pair classification dataset in the same way the model available for inference has been fine-tuned on the QNLI dataset. The model available for fine-tuning attaches a binary classification layer to the Text Embedding model and initializes the layer parameters to random values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c949d5",
   "metadata": {},
   "source": [
    "#### Retrieve Jumpstart Training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef72a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "model_id, model_version = model_dropdown.value, \"*\"\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7edf44",
   "metadata": {},
   "source": [
    "#### Set Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cb3c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "# For a quick demonstration of training we have created a random subset of QNLI dataset.\n",
    "# For complete QNLI dataset replace \"QNLI-tiny\" with \"QNLI\" in the line below.\n",
    "training_data_prefix = \"training-datasets/QNLI-tiny/\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-spc-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9228ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': '3', 'adam-learning-rate': '2e-05', 'batch-size': '64', 'reinitialize-top-layer': 'Auto', 'train-only-top-layer': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"batch-size\"] = \"64\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9579c8",
   "metadata": {},
   "source": [
    "#### Train with Automatic Model Tuning\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a HyperparameterTuner object to interact with Amazon SageMaker hyperparameter tuning APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b5c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = True\n",
    "\n",
    "# Define objective metric per framework, based on which the best model will be selected.\n",
    "metric_definitions_per_model = {\n",
    "    \"tensorflow\": {\n",
    "        \"metrics\": [{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "        \"type\": \"Maximize\",\n",
    "    },\n",
    "    \"huggingface\": {\n",
    "        \"metrics\": [{\"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9\\\\.]+)\"}],\n",
    "        \"type\": \"Maximize\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"adam-learning-rate\": ContinuousParameter(0.000001, 0.001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7dbb04",
   "metadata": {},
   "source": [
    "#### Start Training\n",
    "We start by creating the estimator object with all the required assets and then launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "290ea96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 1 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 39\u001b[0m\n\u001b[1;32m     27\u001b[0m     hp_tuner \u001b[38;5;241m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     28\u001b[0m         spc_estimator,\n\u001b[1;32m     29\u001b[0m         metric_definitions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         base_tuning_job_name\u001b[38;5;241m=\u001b[39mtraining_job_name,\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Launch a SageMaker Tuning job to search for the best hyperparameters\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mhp_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_s3_path\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Launch a SageMaker Training job by passing s3 path of the training data\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     spc_estimator\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_dataset_s3_path}, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:272\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/tuner.py:940\u001b[0m, in \u001b[0;36mHyperparameterTuner.fit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start a hyperparameter tuning job.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \n\u001b[1;32m    883\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m        arguments are needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_with_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_cls_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_with_estimator_dict(inputs, job_name, include_cls_metadata, estimator_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/tuner.py:951\u001b[0m, in \u001b[0;36mHyperparameterTuner._fit_with_estimator\u001b[0;34m(self, inputs, job_name, include_cls_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_estimator_for_tuning(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, inputs, job_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_tuning(job_name\u001b[38;5;241m=\u001b[39mjob_name, include_cls_metadata\u001b[38;5;241m=\u001b[39minclude_cls_metadata)\n\u001b[0;32m--> 951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_tuning_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TuningJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/tuner.py:2013\u001b[0m, in \u001b[0;36m_TuningJob.start_new\u001b[0;34m(cls, tuner, inputs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker HyperParameter Tuning job.\u001b[39;00m\n\u001b[1;32m   1997\u001b[0m \n\u001b[1;32m   1998\u001b[0m \u001b[38;5;124;03mThe new HyperParameter Tuning job uses the provided `tuner` and `inputs`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;124;03m    information about the started job.\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2011\u001b[0m tuner_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tuner_args(tuner, inputs)\n\u001b[0;32m-> 2013\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tuning_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtuner_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tuner\u001b[38;5;241m.\u001b[39msagemaker_session, tuner\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:2412\u001b[0m, in \u001b[0;36mSession.create_tuning_job\u001b[0;34m(self, job_name, tuning_config, training_config, training_config_list, warm_start_config, tags)\u001b[0m\n\u001b[1;32m   2409\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_hyper_parameter_tuning_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m-> 2412\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtune_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tuning_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4813\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   4800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   4801\u001b[0m     \u001b[38;5;28mself\u001b[39m, request: typing\u001b[38;5;241m.\u001b[39mDict, create, func_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   4802\u001b[0m ):\n\u001b[1;32m   4803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   4804\u001b[0m \n\u001b[1;32m   4805\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4811\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   4812\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:2410\u001b[0m, in \u001b[0;36mSession.create_tuning_job.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   2408\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating hyperparameter tuning job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m   2409\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m-> 2410\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_hyper_parameter_tuning_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:960\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 1 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "spc_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    metric_definitions = next(\n",
    "        value for key, value in metric_definitions_per_model.items() if model_id.startswith(key)\n",
    "    )\n",
    "\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        spc_estimator,\n",
    "        metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": training_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    spc_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1da7d6",
   "metadata": {},
   "source": [
    "#### Deploy & run inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d83c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else spc_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2557350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pair1 = [\n",
    "    \"How many octaves does Beyonce have?\",\n",
    "    \"Beyoncé's vocal range spans four octaves.\",\n",
    "]\n",
    "sentence_pair2 = [\n",
    "    \"How many octaves does Beyonce have?\",\n",
    "    \"While another critic says she is a \"\n",
    "    \"Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bf4b0",
   "metadata": {},
   "source": [
    "Next, we query the finetuned model, parse the response and print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text):\n",
    "    response = finetuned_predictor.predict(\n",
    "        encoded_text, {\"ContentType\": \"application/list-text\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response)\n",
    "    probabilities, labels, predicted_label = (\n",
    "        model_predictions[\"probabilities\"],\n",
    "        model_predictions[\"labels\"],\n",
    "        model_predictions[\"predicted_label\"],\n",
    "    )\n",
    "    return probabilities, labels, predicted_label\n",
    "\n",
    "\n",
    "for sentence_pair in [sentence_pair1, sentence_pair2]:\n",
    "    query_response = query_endpoint(json.dumps(sentence_pair).encode(\"utf-8\"))\n",
    "    probabilities, labels, predicted_label = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"Input text: '{sentence_pair}'{newline}\"\n",
    "        f\"Model prediction: {probabilities}{newline}\"\n",
    "        f\"Labels: {labels}{newline}\"\n",
    "        f\"Predicted Label: {bold}{predicted_label}{unbold}{newline}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf4caf",
   "metadata": {},
   "source": [
    "#### Next, we clean up the deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
